{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVxwPIb0gzdsGv4Z80L+kZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/Mydrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ypt1pyTu_U8U","executionInfo":{"status":"ok","timestamp":1727963337379,"user_tz":300,"elapsed":36892,"user":{"displayName":"Mau","userId":"09920865119504042793"}},"outputId":"a359b9f4-c86b-4031-b246-951e13ae8ca5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Mydrive/\n"]}]},{"cell_type":"markdown","source":["Preprocesamiento de imágenes"],"metadata":{"id":"41nVOhHih20Z"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"V_EgovmthcyB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727963351721,"user_tz":300,"elapsed":7367,"user":{"displayName":"Mau","userId":"09920865119504042793"}},"outputId":"7ce396e0-a3fe-4524-e273-d2c673a5b52f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total de parches generados: 0\n"]}],"source":["import cv2\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","\n","# Transformaciones para adaptar las imágenes al modelo ViT\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Redimensionar las imágenes a 224x224\n","    transforms.ToTensor(),  # Convertir las imágenes a tensores\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización estándar\n","])\n","\n","def divide_into_patches(image_path, patch_size=224):\n","    \"\"\"\n","    Divide una imagen grande en parches de tamaño patch_size x patch_size.\n","\n","    Parámetros:\n","        - image_path (str): ruta de la imagen original (aumentada a x40)\n","        - patch_size (int): tamaño de los parches que se generarán (por defecto 224x224)\n","\n","    Devuelve:\n","        - patches (list): una lista con los parches (subimágenes) generados\n","    \"\"\"\n","    # Cargar la imagen\n","    img = cv2.imread(image_path)\n","\n","    # Obtener las dimensiones de la imagen\n","    height, width, _ = img.shape\n","\n","    # Lista para almacenar los parches\n","    patches = []\n","\n","    # Dividir la imagen en parches de patch_size x patch_size\n","    for y in range(0, height, patch_size):\n","        for x in range(0, width, patch_size):\n","            patch = img[y:y + patch_size, x:x + patch_size]\n","            # Asegurarse de que el parche sea exactamente del tamaño especificado\n","            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n","                patches.append(patch)\n","\n","    return patches\n","\n","# Ejemplo de uso:\n","image_path = '/content/Mydrive/MyDrive/IH/9175/1/9175_idx5_x1801_y251_class1.png'\n","patches = divide_into_patches(image_path)\n","\n","# Preprocesar los parches utilizando las transformaciones para ViT\n","patches_tensor = [data_transforms(Image.fromarray(patch)) for patch in patches]\n","\n","print(f\"Total de parches generados: {len(patches_tensor)}\")\n"]},{"cell_type":"markdown","source":["Modelo Vision Transformer con DINO"],"metadata":{"id":"UiMb5vNih7K3"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","\n","# Cargar el modelo ViT-B/8 preentrenado con DINO desde torch.hub\n","model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')\n","\n","# Enviar el modelo a la GPU si está disponible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Crear un tensor de ejemplo para obtener el tamaño de la salida del modelo\n","# El modelo ViT espera una entrada de tamaño (batch_size, 3, 224, 224)\n","dummy_input = torch.randn(1, 3, 224, 224).to(device)\n","\n","# Pasar el tensor de ejemplo a través del modelo para ver la salida\n","model.eval()  # Poner el modelo en modo evaluación\n","with torch.no_grad():\n","    output = model(dummy_input)\n","\n","# Obtener el tamaño de las características de la capa final (número de características de salida)\n","num_ftrs = output.shape[1]  # El tamaño de la salida es (batch_size, num_ftrs)\n","\n","# Cambiar la última capa del modelo (head) para que clasifique 2 clases (IDC negativo o IDC positivo)\n","model.head = nn.Linear(num_ftrs, 2)  # 2 clases: IDC negativo o IDC positivo\n","\n","print(f\"Número de características de la última capa: {num_ftrs}\")\n","print(f\"Modelo Vision Transformer (ViT-B/8) con DINO cargado exitosamente en {device}.\")\n"],"metadata":{"id":"L3uppLkKh7it","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727964267710,"user_tz":300,"elapsed":7406,"user":{"displayName":"Mau","userId":"09920865119504042793"}},"outputId":"4793adb5-006a-4746-9e14-c737051f36cb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"]},{"output_type":"stream","name":"stdout","text":["Número de características de la última capa: 768\n","Modelo Vision Transformer (ViT-B/8) con DINO cargado exitosamente en cpu.\n"]}]},{"cell_type":"markdown","source":["División del conjunto de datos"],"metadata":{"id":"USZswiRfiIIa"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Dividir el conjunto de datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n","X_train, X_test, Y_train, Y_test = train_test_split(patches_tensor, Y_tensor, test_size=0.2, random_state=42)\n","\n","# Convertir a tensores y mover los datos a GPU (si está disponible)\n","X_train, X_test = torch.stack(X_train).to(device), torch.stack(X_test).to(device)\n","Y_train, Y_test = torch.tensor(Y_train).to(device), torch.tensor(Y_test).to(device)\n","\n","# Mostrar las dimensiones del conjunto de datos\n","print(f\"Datos de entrenamiento: {X_train.shape}, Etiquetas de entrenamiento: {Y_train.shape}\")\n","print(f\"Datos de prueba: {X_test.shape}, Etiquetas de prueba: {Y_test.shape}\")\n"],"metadata":{"id":"uz0gZr59iH0f","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1727964312414,"user_tz":300,"elapsed":2681,"user":{"displayName":"Mau","userId":"09920865119504042793"}},"outputId":"3e0a679d-7728-4ea1-a3be-6797f471ac6f"},"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Y_tensor' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fd6bc1a83f1e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Dividir el conjunto de datos en entrenamiento y prueba (80% entrenamiento, 20% prueba)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convertir a tensores y mover los datos a GPU (si está disponible)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Y_tensor' is not defined"]}]},{"cell_type":"markdown","source":["Entrenamiento del modelo"],"metadata":{"id":"vCt1dIauiL-f"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","\n","# Crear dataloaders para manejar los lotes de entrenamiento y prueba\n","train_dataset = TensorDataset(X_train, Y_train)\n","test_dataset = TensorDataset(X_test, Y_test)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Definir el optimizador (Adam) y la función de pérdida (CrossEntropyLoss para clasificación binaria)\n","optimizer = Adam(model.parameters(), lr=1e-4)\n","criterion = CrossEntropyLoss()\n","\n","def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n","    \"\"\"\n","    Función para entrenar el modelo Vision Transformer.\n","\n","    Parámetros:\n","        - model: modelo de Vision Transformer (ViT) preentrenado con DINO.\n","        - train_loader: dataloader que contiene los datos de entrenamiento.\n","        - criterion: función de pérdida (CrossEntropyLoss).\n","        - optimizer: optimizador (Adam).\n","        - num_epochs: número de épocas para entrenar.\n","\n","    Devuelve:\n","        - model: modelo entrenado.\n","    \"\"\"\n","    model.train()  # Poner el modelo en modo de entrenamiento\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        for inputs, labels in tqdm(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()  # Reiniciar los gradientes acumulados\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass y optimización\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Sumar la pérdida y las predicciones correctas\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","        # Calcular la pérdida y precisión por época\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n","\n","        print(f'Epoch {epoch}/{num_epochs-1}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n","\n","    return model\n","\n","# Entrenar el modelo\n","model = train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n"],"metadata":{"id":"hTQT27-7iNXi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluación del modelo"],"metadata":{"id":"Bgk0HLMpiX2m"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","def evaluate_model(model, test_loader):\n","    \"\"\"\n","    Función para evaluar el modelo en el conjunto de prueba.\n","\n","    Parámetros:\n","        - model: modelo Vision Transformer entrenado.\n","        - test_loader: dataloader que contiene los datos de prueba.\n","\n","    Imprime:\n","        - Precisión del modelo, reporte de clasificación y matriz de confusión.\n","    \"\"\"\n","    model.eval()  # Poner el modelo en modo de evaluación\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            # Guardar las predicciones y etiquetas reales\n","            predictions.extend(preds.cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    # Calcular la precisión\n","    accuracy = accuracy_score(true_labels, predictions)\n","    print(f'Precisión del modelo: {accuracy:.4f}')\n","\n","    # Imprimir el reporte de clasificación\n","    print('Reporte de Clasificación:')\n","    print(classification_report(true_labels, predictions))\n","\n","    # Imprimir la matriz de confusión\n","    print('Matriz de Confusión:')\n","    cm = confusion_matrix(true_labels, predictions)\n","    print(cm)\n","\n","# Evaluar el modelo en el conjunto de prueba\n","evaluate_model(model, test_loader)\n"],"metadata":{"id":"SY6C2CMMiYVU"},"execution_count":null,"outputs":[]}]}